# -*- coding: utf-8 -*-
"""Headline_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cn3DIG7UImnHksyuCh2sVpxStMxh4x-U
"""

# ===================== 0. Imports =====================
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras.layers import (
    Input, Dense, Embedding, MultiHeadAttention,
    LayerNormalization, Dropout
)
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split

print("TensorFlow version:", tf.__version__)

# ===================== 1. Load & Clean Data =====================
box_path = "cbb_boxscores_11_2025.csv"
head_path = "cbb_headlines_11_2025.csv"

box = pd.read_csv(box_path)
head = pd.read_csv(head_path)

print("Original boxshape:", box.shape)
print("Original headlines shape:", head.shape)

# Drop bad headlines
head = head[head["headline"] != "No headline found"].copy()
valid_game_ids = head["game_id"].unique()
box = box[box["game_id"].isin(valid_game_ids)].copy()

print("Cleaned headlines shape:", head.shape)
print("Cleaned boxshape:", box.shape)

# ===================== 2. Build Numeric Features (team_id + winner_flag + stats) =====================
if "team" not in box.columns:
    raise ValueError("Expected a 'team' column in boxscores. Rename your team column accordingly.")

# Categorical team ID
box["team_id"] = box["team"].astype("category").cat.codes

# Base numeric stats
numeric_cols = ["team_id", "MIN", "PTS", "REB", "AST", "TO", "STL", "BLK", "OREB", "DREB", "PF"]

# Fill NaNs
for col in numeric_cols:
    if col in box.columns:
        box[col] = box[col].fillna(0.0)
    else:
        raise ValueError(f"Missing expected numeric column '{col}' in boxscores.")

# Game-level scores for winner_flag
game_scores = (
    box.groupby(["game_id", "team_id"])["PTS"]
    .sum()
    .reset_index(name="team_pts")
)

idxmax = game_scores.groupby("game_id")["team_pts"].idxmax()
idxmin = game_scores.groupby("game_id")["team_pts"].idxmin()

winners = game_scores.loc[idxmax, ["game_id", "team_id"]].copy()
winners["winner_flag"] = 1

losers = game_scores.loc[idxmin, ["game_id", "team_id"]].copy()
losers["winner_flag"] = 0

winner_info = pd.concat([winners, losers], ignore_index=True)

# Merge winner_flag onto each player row
box = box.merge(winner_info, on=["game_id", "team_id"], how="left")
box["winner_flag"] = box["winner_flag"].fillna(0).astype("float32")

numeric_cols.append("winner_flag")

print("Numeric feature columns:", numeric_cols)

max_players = 50
num_features = len(numeric_cols)

# ===================== 3. Build Encoder Input X and Text Targets y =====================
X_list = []
y_texts = []

for _, row in head.iterrows():
    gid = row["game_id"]
    headline = row["headline"]

    game_players = box[box["game_id"] == gid].copy()
    game_players = game_players.sort_values("PTS", ascending=False)

    feats = game_players[numeric_cols].values.astype("float32")

    if feats.shape[0] >= max_players:
        feats = feats[:max_players, :]
    else:
        pad_len = max_players - feats.shape[0]
        pad = np.zeros((pad_len, num_features), dtype="float32")
        feats = np.vstack([feats, pad])

    X_list.append(feats)
    y_texts.append(headline)

X = np.stack(X_list)  # (num_games, max_players, num_features)
print("X shape:", X.shape)

# ===================== 4. Tokenize Headlines =====================
start_token = "<start>"
end_token = "<end>"

y_texts_with_tokens = [f"{start_token} {t} {end_token}" for t in y_texts]

tokenizer = Tokenizer(filters="", lower=True, oov_token="<unk>")
tokenizer.fit_on_texts(y_texts_with_tokens)

sequences = tokenizer.texts_to_sequences(y_texts_with_tokens)
max_len = max(len(s) for s in sequences)
print(f"Max headline length (with start/end): {max_len}")

decoder_input_data = []
decoder_target_data = []

for seq in sequences:
    decoder_input_data.append(seq[:-1])   # input to decoder
    decoder_target_data.append(seq[1:])   # shifted target

decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_len - 1, padding="post")
decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_len - 1, padding="post")

decoder_input_data = decoder_input_data.astype("int32")
decoder_target_data = decoder_target_data.astype("int32")

vocab_size = len(tokenizer.word_index) + 1
print("Vocab size:", vocab_size)

# ===================== 5. Train/Validation Split =====================
X_train, X_val, dec_in_train, dec_in_val, dec_tgt_train, dec_tgt_val = train_test_split(
    X, decoder_input_data, decoder_target_data, test_size=0.1, random_state=42
)

print("Train games:", X_train.shape[0], "Val games:", X_val.shape[0])

# ===================== 6. Transformer Building Blocks =====================
def transformer_encoder_block(x, d_model, num_heads, dff, dropout_rate, name_prefix):
    # Self-attention
    attn_output = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=d_model,
        name=f"{name_prefix}_mha"
    )(x, x)
    attn_output = Dropout(dropout_rate, name=f"{name_prefix}_attn_dropout")(attn_output)
    out1 = LayerNormalization(epsilon=1e-6, name=f"{name_prefix}_ln1")(x + attn_output)

    # Feed-forward
    ff = Dense(dff, activation="relu", name=f"{name_prefix}_ff1")(out1)
    ff = Dense(d_model, name=f"{name_prefix}_ff2")(ff)
    ff = Dropout(dropout_rate, name=f"{name_prefix}_ff_dropout")(ff)
    out2 = LayerNormalization(epsilon=1e-6, name=f"{name_prefix}_ln2")(out1 + ff)
    return out2

def transformer_decoder_block(x, enc_output, d_model, num_heads, dff, dropout_rate, name_prefix):
    # Masked self-attention
    self_attn = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=d_model,
        name=f"{name_prefix}_self_mha"
    )(x, x, use_causal_mask=True)
    self_attn = Dropout(dropout_rate, name=f"{name_prefix}_self_attn_dropout")(self_attn)
    out1 = LayerNormalization(epsilon=1e-6, name=f"{name_prefix}_ln1")(x + self_attn)

    # Cross-attention
    cross_attn = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=d_model,
        name=f"{name_prefix}_cross_mha"
    )(out1, enc_output)
    cross_attn = Dropout(dropout_rate, name=f"{name_prefix}_cross_attn_dropout")(cross_attn)
    out2 = LayerNormalization(epsilon=1e-6, name=f"{name_prefix}_ln2")(out1 + cross_attn)

    # Feed-forward
    ff = Dense(dff, activation="relu", name=f"{name_prefix}_ff1")(out2)
    ff = Dense(d_model, name=f"{name_prefix}_ff2")(ff)
    ff = Dropout(dropout_rate, name=f"{name_prefix}_ff_dropout")(ff)
    out3 = LayerNormalization(epsilon=1e-6, name=f"{name_prefix}_ln3")(out2 + ff)
    return out3

# ===================== 7. Build Transformer Encoderâ€“Decoder Model =====================
d_model = 128
num_heads = 4
dff = 256
num_encoder_layers = 2
num_decoder_layers = 2
dropout_rate = 0.2

# ----- Encoder -----
encoder_inputs = Input(shape=(max_players, num_features), name="encoder_inputs")

# Project numeric features into d_model dimension
enc_x = Dense(d_model, name="encoder_dense_proj")(encoder_inputs)

# Add simple positional embeddings (trainable)
enc_positions = tf.range(start=0, limit=max_players, delta=1)
enc_pos_embedding_layer = Embedding(input_dim=max_players, output_dim=d_model, name="encoder_pos_embedding")
enc_pos_encoded = enc_x + enc_pos_embedding_layer(enc_positions)[tf.newaxis, :, :]

# Stack encoder blocks
enc_output = enc_pos_encoded
for i in range(num_encoder_layers):
    enc_output = transformer_encoder_block(
        enc_output,
        d_model=d_model,
        num_heads=num_heads,
        dff=dff,
        dropout_rate=dropout_rate,
        name_prefix=f"encoder_block_{i}"
    )

# ----- Decoder -----
decoder_inputs = Input(shape=(max_len - 1,), name="decoder_inputs")

# Token embeddings
tok_embedding_layer = Embedding(input_dim=vocab_size, output_dim=d_model, name="decoder_tok_embedding")
dec_tok_emb = tok_embedding_layer(decoder_inputs)

# Positional embeddings for decoder
dec_positions = tf.range(start=0, limit=max_len - 1, delta=1)
dec_pos_embedding_layer = Embedding(input_dim=max_len, output_dim=d_model, name="decoder_pos_embedding")
dec_x = dec_tok_emb + dec_pos_embedding_layer(dec_positions)[tf.newaxis, :, :]

# Stack decoder blocks
dec_output = dec_x
for i in range(num_decoder_layers):
    dec_output = transformer_decoder_block(
        dec_output,
        enc_output,
        d_model=d_model,
        num_heads=num_heads,
        dff=dff,
        dropout_rate=dropout_rate,
        name_prefix=f"decoder_block_{i}"
    )

# Final projection to vocab
decoder_logits = Dense(vocab_size, activation="softmax", name="decoder_outputs")(dec_output)

model = Model([encoder_inputs, decoder_inputs], decoder_logits)
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# ===================== 8. Train with Early Stopping =====================
epochs = 40
batch_size = 16

callbacks = [
    EarlyStopping(monitor="val_loss", patience=6, restore_best_weights=True),
    ModelCheckpoint("cbb_headline_transformer_best.keras", monitor="val_loss", save_best_only=True)
]

history = model.fit(
    [X_train, dec_in_train],
    dec_tgt_train,
    validation_data=([X_val, dec_in_val], dec_tgt_val),
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    verbose=1
)

# Save final model & tokenizer
model.save("cbb_headline_transformer.keras")
import pickle
with open("cbb_headline_tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

print("Saved transformer model as cbb_headline_transformer.keras and tokenizer as cbb_headline_tokenizer.pkl")

# ===================== 9. Inference: Top-k Sampling =====================
index_to_word = {idx: word for word, idx in tokenizer.word_index.items()}
start_id = tokenizer.word_index.get(start_token, tokenizer.word_index.get("<unk>"))
end_id = tokenizer.word_index.get(end_token, None)

def sample_top_k(probs, k=30):
    probs = np.asarray(probs)
    top_k_idx = np.argsort(probs)[-k:]
    top_k_probs = probs[top_k_idx]
    top_k_probs = top_k_probs / top_k_probs.sum()
    return int(np.random.choice(top_k_idx, p=top_k_probs))

def generate_headline_for_game_id(game_id, max_steps=None, k=30):
    if max_steps is None:
        max_steps = max_len - 1

    game_players = box[box["game_id"] == game_id].copy()
    if game_players.empty:
        return "[No boxscore rows for this game_id]"

    game_players = game_players.sort_values("PTS", ascending=False)
    feats = game_players[numeric_cols].values.astype("float32")

    if feats.shape[0] >= max_players:
        feats = feats[:max_players, :]
    else:
        pad_len = max_players - feats.shape[0]
        feats = np.vstack([feats, np.zeros((pad_len, num_features), dtype="float32")])

    enc_input = np.expand_dims(feats, axis=0)  # (1, max_players, num_features)

    # Start sequence with <start>
    output_seq = [start_id]

    for _ in range(max_steps):
        dec_input = np.zeros((1, max_len - 1), dtype="int32")
        dec_input[0, :len(output_seq)] = output_seq

        preds = model.predict([enc_input, dec_input], verbose=0)
        probs = preds[0, len(output_seq) - 1]  # distribution for current step

        next_id = sample_top_k(probs, k=k)

        if end_id is not None and next_id == end_id:
            break
        output_seq.append(next_id)

    # Convert IDs to tokens
    words = []
    for idx in output_seq:
        w = index_to_word.get(idx, "")
        if w in [start_token, end_token, "<unk>"]:
            continue
        words.append(w)

    return " ".join(words)

# ===================== 10. Quick Test =====================
example_gid = int(head["game_id"].iloc[0])
true_headline = head[head["game_id"] == example_gid]["headline"].iloc[0]
gen_headline = generate_headline_for_game_id(example_gid)

print("\nExample game_id:", example_gid)
print("True headline:   ", true_headline)
print("Generated headline:", gen_headline)