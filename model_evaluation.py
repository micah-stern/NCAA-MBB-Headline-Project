# -*- coding: utf-8 -*-
"""Model_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p3fVWW86-rOChNIjmcNM0fhtQIHGE_Ey
"""

#pip install rouge-score

"""## Sample Dataset (11/24/25)"""

from datetime import date, timedelta

# ===== CHANGE THIS DATE =====
test_date = date(2025, 11, 18)  # <-- ONLY THIS DAY

print("Collecting game IDs for", test_date)

game_id_df = collect_cbb_game_ids(test_date, test_date)
print(f"Found {len(game_id_df)} games on {test_date}")

meta_df_all, box_df_all = collect_cbb_season_data(game_id_df, delay=0.5)

# Save (exact same format as training)
meta_df_all.to_csv("cbb_headlines_test_11_24_2025.csv", index=False)
box_df_all.to_csv("cbb_boxscores_test_11_24_2025.csv", index=False)

print("Saved test files:")
print("  cbb_headlines_test_11_24_2025.csv")
print("  cbb_boxscores_test_11_24_2025.csv")

# ============================================
# 0. Imports
# ============================================
import numpy as np
import pandas as pd
import tensorflow as tf
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.translate.bleu_score import corpus_bleu
from sklearn.metrics import f1_score
from rouge_score import rouge_scorer

import nltk
nltk.download('punkt')

# ============================================
# 1. Load Test Data (NEW CSVs for 11/24/25)
# ============================================

box_path = "cbb_boxscores_test_11_24_2025.csv"
head_path = "cbb_headlines_test_11_24_2025.csv"

box = pd.read_csv(box_path)
head = pd.read_csv(head_path)

print("Original boxshape:", box.shape)
print("Original headlines shape:", head.shape)

# ðŸ”§ ENSURE BOTH game_id COLUMNS ARE THE SAME TYPE
box["game_id"] = box["game_id"].astype(str)
head["game_id"] = head["game_id"].astype(str)

# Drop bad headlines
head = head[head["headline"] != "No headline found"].copy()

valid_game_ids = head["game_id"].unique()
box = box[box["game_id"].isin(valid_game_ids)].copy()

print("Cleaned headlines shape:", head.shape)
print("Cleaned boxshape:", box.shape)
print("Unique game_ids in head:", len(valid_game_ids))
print("Unique game_ids in box:", box["game_id"].nunique())

# ============================================
# 2. Build Numeric Features Exactly Like Training
# ============================================

# Team ID categorical encoding
box["team_id"] = box["team"].astype("category").cat.codes

numeric_cols = [
    "team_id", "MIN", "PTS", "REB", "AST", "TO",
    "STL", "BLK", "OREB", "DREB", "PF"
]

for col in numeric_cols:
    if col in box.columns:
        box[col] = box[col].fillna(0.0)
    else:
        raise ValueError(f"Missing expected numeric column: {col}")

# Winner flag logic
game_scores = (
    box.groupby(["game_id", "team_id"])["PTS"]
    .sum().reset_index(name="team_pts")
)

idxmax = game_scores.groupby("game_id")["team_pts"].idxmax()
idxmin = game_scores.groupby("game_id")["team_pts"].idxmin()

winners = game_scores.loc[idxmax, ["game_id", "team_id"]].copy()
winners["winner_flag"] = 1

losers = game_scores.loc[idxmin, ["game_id", "team_id"]].copy()
losers["winner_flag"] = 0

winner_info = pd.concat([winners, losers], ignore_index=True)

box = box.merge(winner_info, on=["game_id", "team_id"], how="left")
box["winner_flag"] = box["winner_flag"].fillna(0)

numeric_cols.append("winner_flag")   # Now 12 total

print("Numeric feature columns:", numeric_cols)

# ============================================
# 3. Rebuild Encoder Input X_enc for Each Game
# ============================================

max_players = 50
num_features = len(numeric_cols)
game_ids = []
X_list = []
y_texts = []

for _, row in head.iterrows():
    gid = row["game_id"]
    headline = row["headline"]

    game_players = box[box["game_id"] == gid].copy()
    game_players = game_players.sort_values("PTS", ascending=False)

    feats = game_players[numeric_cols].values.astype("float32")

    if feats.shape[0] >= max_players:
        feats = feats[:max_players]
    else:
        pad_len = max_players - feats.shape[0]
        feats = np.vstack([feats, np.zeros((pad_len, num_features), dtype="float32")])

    X_list.append(feats)
    y_texts.append(headline)
    game_ids.append(gid)

X_enc = np.stack(X_list)  # (N, 50, 12)
print("X_enc shape:", X_enc.shape)

# ============================================
# 4. Load Tokenizer + Model
# ============================================

with open("cbb_headline_tokenizer.pkl", "rb") as f:
    tok = pickle.load(f)

model = tf.keras.models.load_model("cbb_headline_transformer_best.keras")

enc_maxlen = model.input_shape[0][1]
dec_maxlen = model.input_shape[1][1]

print("Encoder maxlen from model:", enc_maxlen)
print("Decoder maxlen from model:", dec_maxlen)

assert enc_maxlen == 50, "Encoder length mismatch!"

# ============================================
# 5. Build Decoder Input (teacher forcing)
# ============================================

start_token = "<start>"
end_token   = "<end>"

y_texts_tok = [f"{start_token} {t} {end_token}" for t in y_texts]
sequences = tok.texts_to_sequences(y_texts_tok)

decoder_input_data = [seq[:-1] for seq in sequences]  # remove last token

X_dec = pad_sequences(decoder_input_data, maxlen=dec_maxlen,
                      padding="post", truncating="post").astype("int32")

print("X_dec shape:", X_dec.shape)

# ============================================
# 6. Predict Headlines
# ============================================

def decode_sequence(seq):
    return " ".join(tok.index_word.get(i, "") for i in seq if i != 0).strip()

raw_preds = model.predict([X_enc, X_dec], verbose=0)
pred_ids = np.argmax(raw_preds, axis=-1)
pred_texts = [decode_sequence(seq) for seq in pred_ids]

# ============================================
# 7. Quantitative Metrics
# ============================================

references = [[t.split()] for t in y_texts]
hypotheses = [p.split() for p in pred_texts]

bleu = corpus_bleu(references, hypotheses)

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
rouge_l = np.mean([
    scorer.score(t, p)['rougeL'].fmeasure
    for t, p in zip(y_texts, pred_texts)
])

flat_true, flat_pred = [], []
for r, h in zip(references, hypotheses):
    t_tokens = r[0]
    p_tokens = h[:len(t_tokens)] + ["<pad>"] * max(0, len(t_tokens) - len(h))
    flat_true.extend(t_tokens)
    flat_pred.extend(p_tokens)
f1 = f1_score(flat_true, flat_pred, average="macro")

exact_match = np.mean([
    y_texts[i].lower().strip() == pred_texts[i].lower().strip()
    for i in range(len(y_texts))
])

print("===== QUANTITATIVE METRICS =====")
print("BLEU:", bleu)
print("ROUGE-L:", rouge_l)
print("F1:", f1)
print("Exact match:", exact_match)

# ============================================
# 8. Qualitative Examples
# ============================================

eval_df = pd.DataFrame({
    "game_id": game_ids,
    "headline": y_texts,
    "pred": pred_texts
})

eval_df["bleu"] = [
    corpus_bleu([[t.split()]], [p.split()])
    for t, p in zip(eval_df["headline"], eval_df["pred"])
]

print("\n===== BEST EXAMPLES =====")
print(eval_df.sort_values("bleu", ascending=False).head(10))

print("\n===== WORST EXAMPLES =====")
print(eval_df.sort_values("bleu", ascending=True).head(10))

# TOP 10 (BEST BLEU)

top10 = eval_df.sort_values("bleu", ascending=False).head(10)

print("\n==================== TOP 10 EXAMPLES (by BLEU) ====================\n")
for _, row in top10.iterrows():
    print(f"GAME: {row['game_id']}")
    print(f"TRUE: {row['headline']}")
    print(f"PRED: {row['pred']}")
    print(f"BLEU: {row['bleu']:.2e}")
    print("-" * 60)


# BOTTOM 10 (WORST BLEU)

bottom10 = eval_df.sort_values("bleu", ascending=True).head(10)

print("\n=================== BOTTOM 10 EXAMPLES (by BLEU) ===================\n")
for _, row in bottom10.iterrows():
    print(f"GAME: {row['game_id']}")
    print(f"TRUE: {row['headline']}")
    print(f"PRED: {row['pred']}")
    print(f"BLEU: {row['bleu']:.2e}")
    print("-" * 60)